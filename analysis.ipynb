{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if PyTorch is installed\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA available:\", cuda_available)\n",
    "\n",
    "# If CUDA is available, print the CUDA version and GPU details\n",
    "if cuda_available:\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "slo_stopwords = ['petindvajsetemu', 'nekakih', 'skorajda', 'kdorkoli', 'osemindevetdesetim', 'osemdesetim', 'morete', 'šestindvajsetih', 'nekoč', 'stotero', 'sami', 'le-taka', 'eden', 'izpod', 'sta', 'trideseta', 'našima', 'tretjih', 'enkratnem', 'kakršnakoli', 'štiriindvajsetimi', 'tisto', 'cel', 'pa', 'trinajstem', 'drugačnega', 'sedemnajstima', 'vsaka', 'nekaki', 'nekateri', 'drugačnim', 'rad', 'težki', 'štiridesetimi', 'želiš', 'dvajset', 'vrh', 'svojega', 'bodita', 'enkratnih', 'tolikšnim', 'iste', 'kolikimi', 'smeš', 'šestimi', 'tisočih', 'petinosemdesetim', 'deseterimi', 'dvojna', 'deseterima', 'stotem', 'četrtim', 'prvimi', 'enaindvajseto', 'troja', 'trojo', 'kakršni', 'dvainšestdesetim', 'skozi', 'moramo', 'takile', 'morale', 'hočeva', 'skozenj', 'maralo', 'dolga', 'nečesa', 'vii', 'štiridesetega', 'marsikaterim', 'istim', 'bržkone', 'šestnajstih', 'nekaterem', 'takemule', 'stoterem', 'sedemindvajsetem', 'svojem', 'le-onimi', 'njegova', 'takega', 'njihovih', 'one', 'jih', 'isto', 'štiristotih', 'nekake', 'le-onim', 'lepo', 'njunem', 'od', 'petindvajset', 'devetega', 'gospod', 'nobenih', 'redko', 'dvaindevetdesetega', 'sedemindvajsetemu', 'nekakšnih', 'mednju', 'dvanajstima', 'vajina', 'nas', 'le-takšno', 'oktober', 'f', 'dvestota', 'katerekoli', 'rada', 'svoja', 'kaka', 'prvi', 'odkar', 'in', 'kakršnem', 'si', 'njegovi', 'katerimkoli', 'pozdravljen', 'kratki', 'tretjemu', 'četudi', 'njiju', 'štiristotimi', 'namesto', 'tretjimi', 'tolikimi', 'morem', 'onem', 'enakemu', 'dvakratnimi', 'kolikšni', 'pome', 'dobro', 'trinajstih', 'vaši', 'nekaterima', 'nad', 'izza', 'dvanajstemu', 'nekakšnega', 'kakršnemkoli', 'stran', 'dvakratnima', 'čimer', 'kogar', 'etc.', 'komu', 'čemer', 'nekoliko', 'cela', 'zmogel', 'dvestotih', 'čeravno', 'njena', 'kakršnega', 'tisočo', 'enaintridesetih', 'bosti', 'sedmo', 'devetdeseti', 'kolikšnih', 'vsakršno', 'devetdesetimi', 'vsi', 'petinosemdesetega', 'deset', 'triintridesetih', 'kakor', 'vajinimi', 'petinosemdesetemu', 'kateregakoli', 'vaše', 'petintridesete', 'drugimi', 'le-takimi', 'petinsedemdeset', 'januar', 'marsikateri', 'vsakršnima', 'drugačni', 'vidva', 'enajst', 'kakem', 'kolikšnem', 'štiriindvajsetima', 'vaših', 'deveto', 'neki', 'enaindvajsetem', 'hotel', 'sedeminšestdeset', 'zmogle', 'le-takšnemu', 'polno', 'kakemu', 'najin', 'kakšno', 'raz', 'mesec', 'peterimi', 'nekoga', 'tridesetemu', 'tisoča', 'trojna', 'skoznjo', 'nikomer', 'štirinajstih', 'prednje', 'se', 'enem', 'petintridesetima', 'take', 'enkratne', 'osemnajstima', 'marsičim', 'njenem', 'tretjima', 'četrtem', 'njegovega', 'zmoreva', 'desetim', 'manj', 'zopet', 'štirinajstima', 'le-tak', 'nama', 'nikakršen', 'deseti', 'name', 'takle', 'hotiva', 'nato', 'morali', 'bila', 'dovolite', 'vso', 'le-takšnima', 'tridesete', 'triindvajseto', 'jesti', 'da', 'kadarkoli', 'sedemindvajseto', 'pozdravljeni', 'kakršna', 'obe', 'triintridesetemu', 'vsakih', 'deseterega', 'sedem', 'viia', 'njegovih', 'tisočega', 'spod', 'mojo', 'po', 'nekakimi', 'stvar', 'maraj', 'zanju', 'sebe', 'komer', 'nazaj', 'troj', 'osemnajstem', 'vašo', 'dvojima', 'le-takšnimi', 'obenj', 'le-one', 'dobesedno', 'marsikatere', 'razen', 'njenim', 'smemo', 'najinega', 'njegovem', 'enajste', 'čezenj', 'dober', 'lepe', 'katerihkoli', 'nedelja', 'temuintemu', 'tolikšnemu', 'vi', 'nobenem', 'katerokoli', 'naši', 'itak', 'nobenimi', 'njegovemu', 'preko', 'petinštirideseta', 'sedemdesetemu', 'devetdeset', 'triintridesete', 'vsemi', 'njegovimi', 'mojega', 'devetdesetem', 'trideseti', 'praviti', 'peteremu', 'kakšne', 'z', 'zame', 'petinštiridesetemu', 'osmi', 'trojega', 'oboj', 'njegovima', 'pripravljen', 'tvoja', 'triindvajsetima', 'more', 'predenje', 'lepa', 'sedmim', 'nekima', 'zavoljo', 'tukaj', 'trojno', 'niti', 'dvanajsti', 'šestnajst', 'trikraten', 'le-onemu', 'nočem', 'takele', 'takale', 'zmogl', 'našem', 'izmed', 'izpred', 'njegove', 'njo', 'kakršnih', 'šeststotimi', 'želela', 'prvih', 'marajva', 'triindvajsetemu', 'nikamor', 'nekih', 'šesto', 'ista', 'želeno', 'vajinim', 'desetima', 'podme', 'vseh', 'zadaj', 'moraš', 'temule', 'viii', 'le-tem', 'bomo', 'enakem', 'šestdesete', 'takšna', 've', 'približno', 'kakšen', 'takemu', 'sedma', 'marata', 'tvojih', 'le-tako', 'le-tista', 'njunemu', 'najinimi', 'vaša', 'sreda', 'katerimi', 'marsikaterimi', 'tj.', 'štiriindvajseta', 'njihove', 'dovolim', 'tiste', 'dvojnimi', 'bo', 'kajti', 'petintridesetim', 'triindvajsetih', 'katerimakoli', 'njunim', 'le-takšni', 'svojih', 'njuno', 'štiridesetima', 'nadme', 'njem', 'česa', 'bolj', 'kolikih', 'drugemu', 'petinpetdeset', 'takšnega', 'več', 'vse', 'le-takšnem', 'vata', 'meniti', 'vanje', 'toliko', 'triinpetdesetim', 'prave', 'obeh', 'le-tisti', 'dvojnem', 'nekakšna', 'osemnajstemu', 'devetemu', 'katerikoli', 'tolikih', 'tista', 'jima', 'koga', 'enega', 'kolikšno', 'vzdolž', 'nekaka', 'skoraj', 'h', 'devetnajstemu', 'šestintridesetim', 'petdeseto', 'istih', 'resda', 'šestih', 'tegale', 'le-tist', 'triinpetdesetimi', 'le-takem', 'enkratnim', 'dvestotega', 'sedmega', 'najinem', 'šestdeseto', 'nekak', 'devetdesetima', 'marajta', 'predme', 'tisočerem', 'vendarle', 'stoti', 'pod', 'peterima', 'enakima', 'enajstimi', 'desetemu', 'katerima', 'februar', 'prava', 'tej', 'dvakratno', 'vsako', 'dvanajstimi', 'petdesetem', 'le-onem', 'čimerkoli', 'tvojega', 'tisočim', 'le-oni', 'kakršnimakoli', 'petinosemdeseto', 'šestnajstima', 'petega', 'mu', 'šeststotim', 'tridesetimi', 'mano', 'nekako', 'dvaindevetdesetih', 'taka', 'trikratnimi', 'sedmem', 'smeti', 'triindvajseta', 'dvojo', 'tisočerima', 'osemdesetemu', 'smem', 'veliki', 'trinajsto', 'kakim', 'trikratne', 'onih', 'november', 'same', 'takimle', 'kamor', 'jem', 'marsikaterem', 'katerem', 'oseminštirideset', 'kake', 'predenj', 'svojo', 'kakršnim', 'dvestote', 'tristotimi', 'šestnajsto', 'lahki', 'tistimi', 'osem', 'drugem', 'takem', 'vsakdo', 'ene', 'kakršnemukoli', 'triinpetdesetih', 'nočejo', 'vsake', 'čezme', 'moj', 'bodisi', 'kakšnemu', 'hočem', 'dvanajst', 'trojnega', 'kakršnokoli', 'smejo', 'trinajstemu', 'trojih', 'triindvajsetim', 'sedmimi', 'sedemdesete', 'dovolil', 'čigavim', 'ob', 'triintridesetima', 'štirideseta', 'enakim', 'iznad', 'smeva', 'n', 'najine', 'nikjer', 'dvajseto', 'devetintridesetih', 'dvestotemu', 'nikakršnem', 'mogel', 'želeti', 'temile', 'tisoč', 'bojda', 'petdeseti', 'zmogla', 'sedemnajstim', 'ravno', 'dvojim', 'enajstim', 'povrh', 'vedno', 'boste', 'iv', 'vanju', 'drugega', 'nekaterega', 'osmimi', 'desetega', 'nekaterih', 'dovoliva', 'deseter', 'nikogar', 'povsod', 'čigavem', 'odprt', 'vsakršni', 'l', 'stot', 'tolika', 'midva', 'petnajsti', 'enakega', 'majhen', 'zelo', 'drugim', 'kjerkoli', 'vajinima', 'pripravljeni', 'bržčas', 'ne', 'mogla', 'stotere', 'petnajstem', 'komur', 'triinšestdesetim', 'devetnajstim', 'tja', 'brez', 'te', 'želita', 'enimi', 'osemnajstim', 'marate', 'vajinih', 'kolikšna', 'drugačno', 'čigavo', 'prvega', 'mojih', 'če', 'velika', 'takemle', 'odprti', 'petinosemdesetih', 'tristotih', 'devetnajstih', 'petemu', 'enajstima', 'nikakršnih', 'osemdeseti', 'šestemu', 'visoki', 'drug', 'trojemu', 'ničimer', 'tvojim', 'želimo', 'petindevetdesetih', 'polni', 'koliki', 'enemu', 'neka', 'tejle', 'poleg', 'devetdesete', 'vsej', 'hotele', 'pogodu', 'okoli', 'le-tistega', 'njenemu', 'istemu', 'istem', 'dvajsetemu', 'precej', 'treh', 'dvanajstem', 'njenega', 'kajne', 'že', 'onima', 'enaindvajsetim', 'petinsedemdesetih', 'zapored', 'petih', 'karkoli', 'trojnem', 'junij', 'morda', 'hočejo', 'tele', 'sedeminšestdesetim', 'le-tistimi', 'le-on', 'pot', 'težka', 'hočeta', 'čemurkoli', 'najinim', 'šestnajsti', 'želele', 'deseto', 'njihovo', 'šestnajste', 'tolikem', 'zmoreta', 'tretji', 'kakršnimi', 'sedeminšestdesetimi', 'g', 'primer', 'dvojno', 'petindvajsete', 'štiriindvajsetim', 'svoj', 'tvoji', 'dve', 'marsičem', 'še', 'marsikaterih', 'hala', 'čemerkoli', 'štirideseto', 'petinštirideseti', 'marala', 'dvema', 'ti', 'morava', 'dveh', 'dvojni', 'moral', 'dvojemu', 'tridesetim', 'zmočiti', 'pogosto', 'čemu', 'vsakega', 'ono', 'petinosemdeseta', 'deseta', 'sedemindvajsetih', 'šestindvajsetimi', 'kakšnimi', 'njenima', 'tolikšno', 'druga', 'nekakšnima', 'tisoče', 'dvakratni', 'čez', 'gor', 'zadnji', 'njegovo', 'nocoj', 'kakima', 'petinštirideset', 'vajine', 'triindvajsetem', 'petindevetdesetim', 'nasproti', 'moram', 'petstotimi', 'kratek', 'šestdesetimi', 'nobene', 'prvem', 'dvojnih', 'mara', 'neko', 'dvainšestdeset', 'petinsedemdesetimi', 'to', 'nista', 'petim', 'petindvajsetega', 'le-takšnim', 'petintrideseto', 'njeno', 'štirinajste', 'nje', 'nedavno', 'kamorkoli', 'četrt', 'zase', 'devetnajstimi', 'mogle', 'trikratnega', 'stoto', 'trikratnemu', 'devetima', 'šestdesetima', 'petinštirideseto', 'devete', 'petek', 'nanju', 'bil', 'enaindvajsetega', 'marec', 'žal', 'naproti', 'sedemstotim', 'iz', 'polna', 'tridesetem', 'naše', 'prva', 'tisočerega', 'vsem', 'vendar', 'njihovima', 'sme', 'bojo', 'zanj', 'svojima', 'nočemo', 'trikratno', 'ga.', 'le-tistih', 'osmo', 'dvaindevetdeseta', 'petdeseta', 'tisoči', 'malo', 'nihče', 'tisočemu', 'preden', 'hočemo', 'sedemnajstih', 'katerih', 'kolika', 'šesta', 'štiristotim', 'včasih', 'osemnajsta', 'temveč', 'tolikega', 'vsak', 'kakšna', 'biti', 'osmim', 'dovolita', 'vam', 'stotim', 'petinštiridesete', 'temale', 'moremo', 'trikratnim', 'njihovemu', 'dvojnemu', 'svojim', 'le-onih', 'petera', 'vašemu', 'velik', 'predvsem', 'vsakimi', 'osemdeset', 'kje', 'c', 'marsikatero', 'ali', 'dvanajsto', 'vašem', 'le-tisto', 'trojnim', 'njegovim', 'nadenj', 'kakršnimikoli', 's', 'le-takšen', 'dvoje', 'moglo', 'tvoj', 'stoteremu', 'vsakršen', 'takole', 'oziroma', 'prazno', 'zmoremo', 'dvojen', 'najsi', 'takšno', 'mi', 'devetdesetih', 'veliko', 'enajsta', 'dvainšestdesetih', 'dovoljena', 'vami', 'tristotega', 'ki', 'hoteli', 'prej', 'kolikima', 'kdaj', 'tisočeri', 'le-temu', 'marsikaterega', 'malce', 'četrtega', 'štiriindvajsete', 'seveda', 'rade', 'osemdeseta', 'čigavima', 'zato', 'kolikšnimi', 'dvajsetim', 'dvaindevetdeseti', 'gospa', 'tisočerih', 'trinajstim', 'tristotem', 'enaintrideset', 'sedemindvajseti', 'tolikšni', 'ia', 'vsaki', 'le-takemu', 'trojne', 'peterega', 'tolike', 'vred', 'trinajste', 'naša', 'triinšestdeset', 'nekemu', 'dovolijo', 'šestem', 'devet', 'vama', 'onega', 'nekakšnemu', 'najinih', 'nečem', 'tolikšnimi', 'trinajstimi', 'šestdeseta', 'tvojimi', 'lahko', 'peter', 'nek', 'česarkoli', 'eno', 'sedmima', 'desete', 'šestim', 'svoje', 'trikratni', 'njim', 'peteri', 'marsikaterima', 'takihle', 'trojen', 'smeta', 'oz.', 'devetdeseto', 'nikar', 'takšnima', 'zmogli', 'dvakratnega', 'petintrideseta', 'so', 'petima', 'nobena', 'devetdesetemu', 'najina', 'devetstotimi', 'njihovimi', 'celo', 'č', 'njihovem', 'potem', 'morajo', 'proti', 'dvaindevetdesetemu', 'šesti', 'vame', 'bi', 'nikakršnimi', 'tristo', 'našimi', 'dvaindvajsetih', 'katerimikoli', 'gotovo', 'le-tiste', 'sto', 'čigavi', 'ponedeljek', 'drugačnima', 'kogarkoli', 'trojima', 'štiridesetim', 'seboj', 'skozme', 'nekakšnem', 'enaindvajsetimi', 'dovolila', 'vsakima', 'nam', 'marali', 'visoka', 'nismo', 'drugačen', 'dvaindevetdeseto', 'trinajsta', 'srednji', 'dvajsete', 'moreva', 'toda', 'peti', 'pravi', 'le-t', 'dva', 'kot', 'devetstotim', 'tema', 'drugačna', 'neke', 'osma', 'želiva', 'osmemu', 'dvaindvajsetim', 'stotega', 'prazna', 'tisočera', 'vajinega', 'petnajste', 'zaradi', 'peterem', 'sploh', 'štiriindvajsetega', 'dovolile', 'dvojega', 'devetnajsto', 'jaz', 'pet', 'petnajstemu', 'visoke', 'petinpetdesetimi', 'čigavimi', 'najinima', 'tretjim', 'le-tistim', 'pote', 'zate', 'neradi', 'sedemindvajsete', 'moji', 'hotimo', 'kratke', 'sedemdeseta', 'čeprav', 'nikomur', 'sicer', 'kolikšen', 'enaindvajsetih', 'stotera', 'kako', 'takegale', 'saj', 'osemnajstega', 'no', 'j', 'tretjo', 'triintrideseta', 'mojemu', 'nikakršno', 'petdesetih', 'vase', 'petinosemdeseti', 'v', 'devetintridesetim', 'hoti', 'kakršnima', 'tvojem', 'petimi', 'kolik', 'ampak', 'našemu', 'jo', 'kakršne', 'nerad', 'nikakršnima', 'kom', 'tisočerimi', 'štirinajst', 'dvakratne', 'trojnima', 'petstotim', 'izven', 'želim', 'takšnimi', 'mar', 'niso', 'triintridesetimi', 'čim', 'tretja', 'trideset', 'nanje', 'kjer', 'štiriindvajseti', 'ste', 'le', 'petindvajseto', 'svojemu', 'peterim', 'triintridesetega', 'enkratnimi', 'dvanajsta', 'njima', 'dovoljene', 'vsakršnem', 'nikdar', 'pripravljena', 'drugo', 'bosta', 'nekakšno', 'peta', 'tisočimi', 'nisva', 'pravzaprav', 'smo', 'nočeva', 'osmima', 'dolg', 'nekatero', 'vsakršna', 'blizu', 'temu', 'tolikšnih', 'enkratnemu', 'morati', 'onstran', 'osmega', 'vsega', 'malone', 'dokler', 'osemnajstimi', 'sedmemu', 'drugačnimi', 'obme', 'trojem', 'štirinajstem', 'štiri', 'petinpetdesetim', 'osemdesetega', 'vrhu', 'nečemu', 'tolikšnima', 'čigava', 'šestnajstimi', 'njun', 'tolik', 'enajstih', 'takimile', 'ja', 'kateremukoli', 'kateri', 'maram', 'noben', 'triintrideseto', 'šestdesetega', 'dvajsetem', 'katerim', 'navkljub', 'sedme', 'le-ono', 'nami', 'sedemdesetem', 'četrtih', 'taki', 'običajno', 'vajin', 'daleč', 'moči', 'petnajstima', 'devetnajsta', 'ž', 'petdesete', 'šestnajstem', 'vsakršnim', 'tretjem', 'zgolj', 'sedeminpetdesetim', 'julij', 'prek', 'hočeš', 'i', 'želene', 'predse', 'tristoto', 'stoterih', 'prvo', 'komaj', 'r', 'dvakratnemu', 'le-onega', 'le-takšna', 'bova', 'želi', 'čigave', 'smete', 'slab', 'devetnajsti', 'kolikšnemu', 'vsakršnimi', 'tistima', 'želijo', 'štirinajstega', 'sama', 'koliko', 'oni', 'petintridesetega', 'našega', 'naš', 'šeststotih', 'všeč', 'takim', 'mojimi', 'dvojnega', 'zaprta', 'štiristo', 'petinsedemdesetim', 'ponje', 'enima', 'dolgi', 'hotite', 'petdesetima', 'le-takima', 'nikakršni', 'kolikšne', 'ves', 'triinšestdesetimi', 'njenih', 'želena', 'dvestotima', 'nase', 'mojim', 'enaindvajset', 'vašim', 'april', 'medtem', 'devetsto', 'prav', 'tisti', 'sedemindvajsetim', 'tretjega', 'nekimi', 'devetdeseta', 'dvajsetega', 'sedemindvajset', 'kljub', 'tvojemu', 'kakšni', 'trojim', 'čigavemu', 'petindvajseta', 'takšne', 'šestintridesetih', 'moč', 'moreta', 'sedemdeseti', 'devetnajstima', 'onimi', 'dvestoto', 'sobota', 'pač', 'bili', 'le-tistemu', 'nekem', 'smel', 'petinštiridesetih', 'skoz', 'štiridesetemu', 'do', 'za', 'dovoliš', 'sedemindvajseta', 'zlasti', 'maj', 'enkratno', 'dvakraten', 'šestega', 'tem', 'mene', 'onkraj', 'dvakratnih', 'tristotim', 'sedemnajstega', 'marale', 'nič', 'kadar', 'osemindevetdeset', 'morala', 'eni', 'peterih', 'niste', 'trije', 'šestnajstemu', 'devetstotih', 'tristoti', 'tisočerim', 'petnajstih', 'idr.', 'u', 'tolikšne', 'tremi', 'oseminštiridesetimi', 'sedemindvajsetimi', 'njeni', 'stotih', 'triindvajseti', 'petindvajsetima', 'petindvajsetim', 'enajstega', 'enaka', 'triintridesetem', 'trojnih', 'osmem', 'tisočeremu', 'sedemdesetimi', 'osemdesete', 'drugačnem', 'nekatera', 'zanje', 'le-oniti', 'kolikim', 'samo', 'ko', 'tvojo', 'želen', 'torek', 'hotela', 'sedemnajstem', 'štiriindvajsetemu', 'sedemdesetega', 'trideseto', 'štirimi', 'devetdesetim', 'štirinajstim', 'medme', 'triindvajsete', 'sedaj', 'ponjo', 'najino', 'danes', 'edinole', 'triindvajsetimi', 'petstotih', 'dvojnim', 'le-tej', 'kakršnekoli', 'želelo', 'vsa', 'dvaindevetdesetim', 'tega', 'dvaindevetdesetima', 'triintrideseti', 'kdo', 'trikratna', 'dovolili', 'deveti', 'čigar', 'mojima', 'zaprti', 'npr.', 'kakšnim', 'naj', 'njihovi', 'zakaj', 'najinemu', 'katera', 'dvanajstega', 'dvojih', 'osemnajsto', 'nekakšni', 'nji', 'e', 'nadnje', 'stoterima', 'šestnajstim', 'petintridesetimi', 'iia', 'dol', 'dovolimo', 'štirinajstimi', 'šestdesetemu', 'osemdeseto', 'petintrideseti', 'enaindvajsetemu', 'troje', 'sedemdesetih', 'osemnajsti', 'poln', 'vašima', 'enih', 'petnajst', 'tistem', 'visok', 'četrtimi', 'boš', 'marsikaj', 'le-ona', 'le-temi', 'trinajst', 'dvajsetih', 'nisem', 'vsakem', 'petindevetdeset', 'kakšnega', 'ker', 'toliki', 'ničemur', 'dvajsetima', 'zoper', 'kajpada', 'želeni', 'devetintridesetimi', 'vate', 'nekakima', 'zmorejo', 'njene', 'morejo', 'tistemu', 'dvakratna', 'leto', 'nekaj', 'nekomu', 'štirim', 'zunaj', 'enaki', 'š', 'dvainšestdesetimi', 'dvestotem', 'osemnajstih', 'ix', 'morate', 'enim', 'marsičemu', 'takšnih', 'dvoj', 'enajstemu', 'njen', 'domala', 'petindvajsetem', 'tistega', 'takšen', 'vanj', 'onemu', 'kaj', 'nikakršne', 'nekakega', 'desetih', 'njunima', 'osemdesetem', 'dvojne', 'pribl.', 'dvaindevetdesetem', 'tristotemu', 'stote', 'onedve', 'štiridesetem', 'vštric', 'vajinemu', 'temuintem', 'marsikateremu', 'onadva', 'tistih', 'odprta', 'petinpetdesetih', 'kakšnima', 'njunih', 'marsičesa', 'tehle', 'prvim', 'petero', 'nekdo', 'marveč', 'zdaj', 'štiriindvajsetem', 'deveta', 'tridesetima', 'marajo', 'dvoji', 'morebiti', 'vsakršne', 'pri', 'včeraj', 'tolikim', 'triindvajset', 'koderkoli', 'marava', 'istega', 'tistim', 'druge', 'šestnajstega', 'smele', 'enak', 'istima', 'kakršnihkoli', 'hotelo', 'enajsti', 'meni', 'pravo', 'triintrideset', 'vajini', 'ona', 'oseminštiridesetim', 'štirinajsto', 'medse', 'zmorete', 'september', 'dovoljen', 'petdeset', 'le-ti', 'drugi', 'avgust', 'njihov', 'prednji', 'halo', 'le-teh', 'dvanajste', 'dvaindevetdeset', 'onim', 'enaindvajsetima', 'šestdesetem', 'petinosemdesetem', 'kolikšnim', 'petindvajsetimi', 'štirinajsti', 'vsakršnega', 'dvanajstim', 'oboje', 'dvajsetimi', 'nočeš', 'sam', 'ji', 'namreč', 'šestdeset', 'tretje', 'šestima', 'midve', 'takima', 'drugačnemu', 'petnajsto', 'stotemu', 'kakih', 'tridesetega', 'desetero', 'nisi', 'trinajsti', 'sedemdesetim', 'mimo', 'sedeminšestdesetih', 'enake', 'naju', 'enkrat', 'trojnimi', 'le-takim', 'način', 'petindevetdesetimi', 'sedmi', 'četrtemu', 'ponavadi', 'zmorem', 'le-takšnih', 'deseterih', 'vajinem', 'bodite', 'b', 'petnajstim', 'petinosemdesetimi', 'spričo', 'zaprto', 'devetimi', 'mora', 'komerkoli', 'petnajstega', 'moralo', 'zanjo', 'štiridesete', 'via', 'petinštiridesetem', 'želel', 'nekakšnimi', 'deseteremu', 'nekakšen', 'petinosemdeset', 'dvanajstih', 'čeznje', 'dan', 'nemara', 'osemindevetdesetimi', 'drugih', 'bile', 'četrto', 'dovoljeni', 'bilo', 'tak', 'marsikom', 'na', 'kakšnih', 'ta', 'nobenim', 'dovoli', 'onidve', 'devetnajstem', 'vsakim', 'njih', 'deseteri', 'enakimi', 'trikratnem', 'stoterim', 'takimale', 'četrtek', 'tolikima', 'petnajsta', 'dvestotimi', 'le-toliko', 'dovoljeno', 'četrte', 'šeste', 'dno', 'navzlic', 'prazen', 'sedemdeseto', 'največ', 'ju', 'sedeminpetdesetimi', 'njenimi', 'vaju', 'enaindvajseta', 'vas', 'dvestotim', 'njuni', 'bodi', 'prve', 'm', 'res', 'vaš', 'koli', 'marsikomu', 'tolikšnem', 'ponovno', 'podnjo', 'dvakratnim', 'petindvajsetih', 'petdesetemu', 'kolikor', 'katerega', 'marajte', 'koder', 'en', 'krog', 'k', 'desetere', 'le-tistem', 'le-take', 'tristote', 'isti', 'vsakemu', 'njegov', 'desetem', 'bodimo', 'naokoli', 'katero', 'enako', 'petintridesetih', 'osemdesetih', 'lepi', 'štirinajstemu', 'verjetno', 'šestindvajsetim', 'marati', 'kakršnemu', 'nekateremu', 'dobra', 'oseminštiridesetih', 'zadosti', 'tisočima', 'drugačnih', 'mojem', 'tole', 'kakimi', 'bodo', 'drugačne', 'kakršno', 'morata', 'ii', 'četrtima', 'šestdeseti', 'sedmih', 'smeli', 'zaprt', 'petinosemdesete', 'mnogo', 'devetintrideset', 'tisočero', 'le-takih', 'komurkoli', 'vsakomur', 'devetnajst', 'sedemnajstimi', 'sedemindvajsetega', 'sedemindvajsetima', 'nadvse', 'noče', 'dvojnima', 'nikakršnega', 'naših', 'dvaindvajsetimi', 'trojnemu', 'vsakršnemu', 'tebe', 'njuna', 'svojimi', 'ni', 'tolikemu', 'torej', 'kakega', 'datum', 'devetih', 'sedemdesetima', 'prvima', 'štiriindvajset', 'petinštiridesetim', 'tristotima', 'zmore', 'triinšestdesetih', 'le-takšne', 'sva', 'menda', 'le-tistima', 'tolikšna', 'nikakršnim', 'enaindvajsete', 'osemnajste', 'vsakomer', 'šestindvajset', 'nobenima', 'teh', 'kolikega', 'kolikem', 'maramo', 'takih', 'četrta', 'dvajseti', 'takšnim', 'o', 'nekatere', 'hočete', 'lahka', 'kakršenkoli', 'menoj', 'obema', 'sedemnajsto', 'deseterem', 'nate', 'sedemnajstemu', 'kdor', 'zmoreš', 'nikakršnemu', 'nekakemu', 'takšnem', 'svoji', 'sedemdeset', 'dvojimi', 'teboj', 'sedemnajst', 'jutri', 'le-taki', 'zmoči', 'tebi', 'istimi', 'marsikoga', 'vsakršnih', 'štirih', 'on', 'nobenega', 'narobe', 'osemdesetima', 'tabo', 'trojimi', 'tisočem', 'kolike', 'našim', 'enkratnima', 'petindvajseti', 'sabo', 'marsikatera', 'temi', 'je', 'vnovič', 'osemnajst', 'nekje', 'desetera', 'stoterimi', 'nekom', 'enkratni', 'vsemu', 'kolikšnega', 'le-takšnega', 'njimi', 'njune', 'povrhu', 'triindvajsetega', 'sedeminpetdeset', 'njihovega', 'skoznje', 'petinštiridesetega', 'kaki', 'vašimi', 'le-teti', 'moja', 'stoteri', 'okrog', 'šestdesetim', 'kak', 'le-te', 'kakršnikoli', 'nekakšnim', 'dovolilo', 'dovoliti', 'čemur', 'ničesar', 'štirideseti', 'le-ta', 'le-takega', 'zraven', 'jim', 'njunimi', 'stotima', 'marajmo', 'štiriindvajsetih', 'nočeta', 'dvojem', 'takimi', 'petdesetega', 'hoteti', 'tristota', 'ponj', 'takšni', 'december', 'baje', 'prbl.', 'le-onima', 'd', 'tam', 'njemu', 'trikratnima', 'majhna', 'devetim', 'petdesetimi', 'tu', 'osemdesetimi', 'njihova', 'p', 'petnajstimi', 'dovolj', 'šeststo', 'šestdesetih', 'enaindvajseti', 'tudi', 'petsto', 'pred', 'sebi', 'lahek', 'njega', 'maral', 'našo', 'takšnemu', 'enakih', 'nikakršna', 'nekakim', 'želeli', 'težek', 'le-tega', 'sedemnajsta', 'prvemu', 'le-to', 'najmanj', 'petintrideset', 'reč', 'sedemstotimi', 'enkratna', 'moreš', 'med', 'trinajstima', 'nanjo', 'mednje', 'kakršen', 'kajpak', 'četrti', 'nekakem', 'tri', 'petinštiridesetima', 'šestintridesetimi', 'vanjo', 'iva', 'petem', 'enaintridesetimi', 'petinosemdesetima', 'petdesetim', 'hoče', 'ničemer', 'peto', 'celi', 'oba', 'drugima', 'katere', 'vedeti', 'štirinajsta', 'smelo', 'čem', 'enkratnega', 'kateremu', 'kar', 'tridesetih', 'dvoja', 'kakšnem', 'enkraten', 'osemindevetdesetih', 'dvajseta', 'vsema', 'čigavih', 'a', 'pete', 'tvojima', 'štirje', 'moje', 'vsaj', 'zares', 'vsakogar', 'vašega', 'nobeno', 'njej', 'me', 'česar', 'lahke', 'ena', 'meja', 'vajino', 'štiridesetih', 'le-tema', 'kolikemu', 'sedemnajste', 'stota', 'x', 'sedemstotih', 'nekakšne', 'smela', 'stoterega', 'trojni', 'tvoje', 'trikratnih', 'nikoli', 'dobri', 'šestnajsta', 'desetimi', 'čigavega', 'osme', 'bodiva', 'devetnajstega', 'želite', 'nekaterimi', 'tisočere', 'čigav', 'nekaterim', 'iii', 'mej', 'težak', 'štirideset', 'enajsto', 'hotita', 'petintridesetemu', 'tako', 'nekega', 'tolikšen', 'kolikšnima', 'dvesto', 'dvaindevetdesetimi', 'radi', 'devetdesetega', 'nočete', 'enaintridesetim', 'kakršnimkoli', 't', 'stoter', 'sedemnajsti', 'tolikšnega', 'majhni', 'g.', 'nekim', 'smet', 'spet', 'petere', 'dvestoti', 'srednja', 'deseterim', 'bom', 'petintridesetem', 'tale', 'šele', 'često', 'triinpetdeset', 'nečim', 'sem', 'njihovim', 'takoj', 'nanj', 'sedeminpetdesetih', 'sredi', 'tile', 'preprosto', 'težko', 'kam', 'dvaindevetdesete', 'devetem', 'maraš', 'najbrž', 'dvakratnem', 'petinštiridesetimi', 'katerakoli', 'kakršnegakoli', 'najini', 'nobenemu', 'mogli', 'vpričo', 'kratka', 'šestintrideset', 'ga', 'devetnajste', 'marsikdo', 'ter', 'štiriindvajseto', 'šest', 'lep', 'trinajstega', 'njunega', 'troji', 'enajstem', 'temle', 'nobeni', 'skozte', 'triintridesetim', 'tisočer', 'osmih', 'sedemsto', 'kateremkoli', 'trem', 'dvaindvajset', 'stotimi']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_csv_with_embeddings(file_path, embedding_column=\"embedding\"):\n",
    "\n",
    "    def convert_embedding(embedding_str):\n",
    "        return np.array([float(x) for x in embedding_str.strip('[]').split()])\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    df[embedding_column] = df[embedding_column].apply(convert_embedding)\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALSO DOES SENTIMENT ANALYSIS\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "nlp = spacy.load(\"sl_core_news_trf\")\n",
    "\n",
    "# Function to process text and extract entities, lemmatized text, and count entities\n",
    "def process_text(doc):\n",
    "    print(\"+\")\n",
    "    entities = list({(ent.text, ent.label_) for ent in doc.ents})\n",
    "    \n",
    "    # Lemmatize and remove non-alphabetic characters and stopwords\n",
    "    lemmatized_text = \" \".join([token.lemma_ for token in doc if token.is_alpha and token.text.lower() not in slo_stopwords])\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    lemmatized_text = re.sub(r'[^\\w\\s]', '', lemmatized_text).lower().strip()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    lemmatized_text = re.sub(r'\\s+', ' ', lemmatized_text)\n",
    "    \n",
    "    # Count entity types\n",
    "    ne_loc_cnt = sum(1 for _, label in entities if label in {'GPE', 'LOC'})\n",
    "    ne_per_cnt = sum(1 for _, label in entities if label == 'PER')\n",
    "    ne_org_cnt = sum(1 for _, label in entities if label == 'ORG')\n",
    "    ne_misc_cnt = sum(1 for _, label in entities if label == 'MISC')\n",
    "    print(\"+\")\n",
    "\n",
    "    return entities, lemmatized_text, ne_loc_cnt, ne_per_cnt, ne_org_cnt, ne_misc_cnt\n",
    "\n",
    "\n",
    "\n",
    "# Define one hour in seconds\n",
    "one_hour_in_seconds = 3600\n",
    "\n",
    "# Function to count articles in one-hour interval for each category\n",
    "def count_articles_in_interval(timestamp, timestamps, topics, category, interval=one_hour_in_seconds):\n",
    "    start_interval = timestamp - interval\n",
    "    end_interval = timestamp + interval\n",
    "    return np.sum((timestamps >= start_interval) & (timestamps <= end_interval) & (topics == category))\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(df,output_name,):\n",
    "    if output_name == \"train_df.csv\":\n",
    "        df = df.dropna(subset=['topics'])\n",
    "    ##DROP CATEGORY if exists\n",
    "    if 'category' in df.columns:\n",
    "        df = df.drop(columns=['category'])\n",
    "    \n",
    "    # Check if CUDA is available and set the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Print GPU and CUDA details if available\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EMBEDDIA/sloberta\")\n",
    "    model = AutoModel.from_pretrained(\"EMBEDDIA/sloberta\").to(device)\n",
    "    #print model size\n",
    "    def combine_columns(row):\n",
    "        title = row['title']\n",
    "        lead = row['lead']\n",
    "        paragraphs = \" \".join(row['paragraphs']) if isinstance(row['paragraphs'], list) else row['paragraphs']\n",
    "        keywords = \", \".join(row['keywords']) if isinstance(row['keywords'], list) else row['keywords']\n",
    "        gpt_keywords = \", \".join(row['gpt_keywords']) if isinstance(row['gpt_keywords'], list) else row['gpt_keywords']\n",
    "        \n",
    "        combined_text = f\"Title: {title}\\n\\nLead: {lead}\\n\\nContent: {paragraphs}\\n\\nKeywords: {keywords}\\n\\nGPT Keywords: {gpt_keywords}\"\n",
    "        return combined_text\n",
    "    \n",
    "    def category_calculation(df):\n",
    "    # Load the DataFrame\n",
    "    \n",
    "        # Function to extract categories from the URL\n",
    "        def extract_category(url):\n",
    "            # Split the URL on \"/\"\n",
    "            parts = url.split('/')\n",
    "            # Find the index for \".si\"\n",
    "            index = parts.index('www.rtvslo.si') + 1\n",
    "            # Return the next two parts joined by \"/\"\n",
    "            return '/'.join(parts[index:index+2])\n",
    "\n",
    "        # Apply the function to the 'url' column\n",
    "        df['category_new'] = df['url'].apply(extract_category)\n",
    "\n",
    "        # Count the unique values in the 'category' column\n",
    "        category_counts = df['category_new'].value_counts()\n",
    "\n",
    "        # Calculate the 0.5% threshold of the total dataset size\n",
    "        threshold = len(df) * 0.005\n",
    "\n",
    "        # Filter categories that appear more than the threshold\n",
    "        significant_categories = category_counts[category_counts > threshold].index.tolist()\n",
    "\n",
    "        # Define a function to adjust category based on significance\n",
    "        def adjust_category(category):\n",
    "            # Check if the category is significant\n",
    "            if category not in significant_categories:\n",
    "                # If not, return the higher-level hierarchy\n",
    "                return category.split('/')[0]\n",
    "            else:\n",
    "                # If it is, return the category as is\n",
    "                return category\n",
    "\n",
    "        # Apply the function to adjust categories\n",
    "        df['category_new'] = df['category_new'].apply(adjust_category)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def combine_columns_lead_title(row):\n",
    "        title = row['title']\n",
    "        lead = row['lead']  \n",
    "        keywords = \", \".join(row['keywords']) if isinstance(row['keywords'], list) else row['keywords']\n",
    "        gpt_keywords = \", \".join(row['gpt_keywords']) if isinstance(row['gpt_keywords'], list) else row['gpt_keywords']\n",
    "\n",
    "        combined_text = f\"{title} {lead} {keywords} {gpt_keywords}\"\n",
    "        return combined_text\n",
    "    \n",
    "    \n",
    "    # Function to split text into chunks\n",
    "    def split_into_chunks(text, chunk_size=512, overlap=50):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), chunk_size - overlap):\n",
    "            chunk = tokens[i:i + chunk_size]\n",
    "            chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "        return chunks\n",
    "\n",
    "    # Function to compute embeddings\n",
    "    def compute_embedding(text):\n",
    "        chunks = split_into_chunks(text)\n",
    "        chunk_embeddings = []\n",
    "        for chunk in chunks:\n",
    "            inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            chunk_embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy())\n",
    "        print(\"-\")\n",
    "        return np.mean(chunk_embeddings, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df['combined_text'] = df.apply(combine_columns, axis=1)\n",
    "    df['combined_text_lead_title'] = df.apply(combine_columns_lead_title, axis=1)\n",
    "\n",
    "    time_start = time.time()\n",
    "    # Compute embeddings for the combined text\n",
    "    df['embedding'] = df['combined_text'].apply(compute_embedding)\n",
    "    time_end = time.time()\n",
    "    print(\"Time taken to compute embeddings:\", time_end - time_start, \"seconds\")\n",
    "    \n",
    "##TIME TO SIN COS\n",
    "        # Convert to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Convert to Unix timestamp\n",
    "    df['unix_timestamp'] = df['date'].astype(int) / 10**9\n",
    "\n",
    "    # Normalize by subtracting the minimum value\n",
    "    min_timestamp = df['unix_timestamp'].min()\n",
    "    df['unix_timestamp'] = df['unix_timestamp'] - min_timestamp\n",
    "\n",
    "    # Define the periods (in seconds) for different cycles\n",
    "    seconds_in_day = 24 * 60 * 60\n",
    "    seconds_in_week = 7 * seconds_in_day\n",
    "    seconds_in_month = 30 * seconds_in_day  # Approximation\n",
    "    seconds_in_quarter = 3 * seconds_in_month\n",
    "    seconds_in_half_year = 6 * seconds_in_month\n",
    "    seconds_in_year = 12 * seconds_in_month\n",
    "\n",
    "    # Apply sine and cosine transformations for different cycles\n",
    "    # Daily cycle\n",
    "    df['daily_sin'] = np.sin(2 * np.pi * df['unix_timestamp'] / seconds_in_day)\n",
    "    df['daily_cos'] = np.cos(2 * np.pi * df['unix_timestamp'] / seconds_in_day)\n",
    "\n",
    "    # Three-day cycle\n",
    "    df['three_day_sin'] = np.sin(2 * np.pi * df['unix_timestamp'] / (3 * seconds_in_day))\n",
    "    df['three_day_cos'] = np.cos(2 * np.pi * df['unix_timestamp'] / (3 * seconds_in_day))\n",
    "\n",
    "    # Weekly cycle\n",
    "    df['weekly_sin'] = np.sin(2 * np.pi * df['unix_timestamp'] / seconds_in_week)\n",
    "    df['weekly_cos'] = np.cos(2 * np.pi * df['unix_timestamp'] / seconds_in_week)\n",
    "\n",
    "    # Monthly cycle\n",
    "    df['monthly_sin'] = np.sin(2 * np.pi * df['unix_timestamp'] / seconds_in_month)\n",
    "    df['monthly_cos'] = np.cos(2 * np.pi * df['unix_timestamp'] / seconds_in_month)\n",
    "\n",
    "    # Quarterly cycle\n",
    "    df['quarterly_sin'] = np.sin(2 * np.pi * df['unix_timestamp'] / seconds_in_quarter)\n",
    "    df['quarterly_cos'] = np.cos(2 * np.pi * df['unix_timestamp'] / seconds_in_quarter)\n",
    "\n",
    "    # Half-year cycle\n",
    "    df['half_yearly_sin'] = np.sin(2 * np.pi * df['unix_timestamp'] / seconds_in_half_year)\n",
    "    df['half_yearly_cos'] = np.cos(2 * np.pi * df['unix_timestamp'] / seconds_in_half_year)\n",
    "\n",
    "    ##NUMBER OF FIGURES\n",
    "    df['figure_count'] = df['figures'].apply(len)\n",
    "    \n",
    "    ##NUMBER OF PARAGRAPHS\n",
    "    df['paragraph_count'] = df['paragraphs'].apply(len)\n",
    "\n",
    "    \n",
    "    ##ARTICLE LENGTH\n",
    "    df['article_word_count'] = df['combined_text'].apply(lambda text: len(text.split()))\n",
    "    \n",
    "    ##TITLE LENGTH\n",
    "    df['title_word_count'] = df['title'].apply(lambda text: len(text.split()))\n",
    "    #DOES TITLE CONTAIN ! OR ?  Create features indicating the presence of '?' and '!' in the title\n",
    "    df['artquestion'] = df['title'].apply(lambda x: '?' in x)\n",
    "    df['artexclaim'] = df['title'].apply(lambda x: '!' in x)\n",
    "    \n",
    "    #ARTICLES IN ONE HOUR    \n",
    "\n",
    "    try:\n",
    "        # Try to open the file and load the list\n",
    "        with open(\"unique_col.json\", 'r') as file:\n",
    "            unique_topics = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, create the list and save it\n",
    "        unique_topics = list(df['topics'].unique())\n",
    "        with open(\"unique_col.json\", 'w') as file:\n",
    "            json.dump(unique_topics, file)\n",
    "        \n",
    "    \n",
    "    # Iterate over each unique topic and calculate the count in the one-hour interval\n",
    "    for topic in unique_topics:\n",
    "        df[f'{topic}_count_in_one_hour'] = df.apply(\n",
    "            lambda row: count_articles_in_interval(row['unix_timestamp'], df['unix_timestamp'], df['topics'], topic),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    timestart = time.time()\n",
    "    # Apply functions to count different types of entities\n",
    "    # Process the text and save entities\n",
    "    # Apply the NLP pipeline and process the text\n",
    "    df['doc'] = df['combined_text_lead_title'].apply(nlp)\n",
    "    df[['entities', 'lemmatized_lead_title', 'ne_loc_cnt', 'ne_per_cnt', 'ne_org_cnt', 'ne_misc_cnt']] = df['doc'].apply(lambda doc: pd.Series(process_text(doc)))\n",
    "\n",
    "\n",
    "    timeend = time.time()\n",
    "    print(\"Time taken to count entities-Spacy bert task:\", timeend - timestart, \"seconds\")\n",
    "\n",
    "    #CATEGORY NEW CALCULATION\n",
    "    category_calculation(df)\n",
    "\n",
    "\n",
    "    #THE SAME df BUT SORT COLUMNS ACCORDING TO THE ALPHABET\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    df.to_csv(output_name, index=False) \n",
    "    return df\n",
    "\n",
    "    #save train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLD IMPLEMENTATION, BUT IS USED NOW\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "nlp = spacy.load(\"sl_core_news_trf\")\n",
    "\n",
    "# Function to process text and extract entities, lemmatized text, and count entities\n",
    "def process_text(doc):\n",
    "    print(\"+\")\n",
    "    entities = list({(ent.text, ent.label_) for ent in doc.ents})\n",
    "    \n",
    "    # Lemmatize and remove non-alphabetic characters and stopwords\n",
    "    lemmatized_text = \" \".join([token.lemma_ for token in doc if token.is_alpha and token.text.lower() not in slo_stopwords])\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    lemmatized_text = re.sub(r'[^\\w\\s]', '', lemmatized_text).lower().strip()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    lemmatized_text = re.sub(r'\\s+', ' ', lemmatized_text)\n",
    "    \n",
    "    # Count entity types\n",
    "    ne_loc_cnt = sum(1 for _, label in entities if label in {'GPE', 'LOC'})\n",
    "    ne_per_cnt = sum(1 for _, label in entities if label == 'PER')\n",
    "    ne_org_cnt = sum(1 for _, label in entities if label == 'ORG')\n",
    "    ne_misc_cnt = sum(1 for _, label in entities if label == 'MISC')\n",
    "    print(\"+\")\n",
    "\n",
    "    return entities, lemmatized_text, ne_loc_cnt, ne_per_cnt, ne_org_cnt, ne_misc_cnt\n",
    "\n",
    "\n",
    "\n",
    "# Define one hour in seconds\n",
    "one_hour_in_seconds = 3600\n",
    "\n",
    "# Function to count articles in one-hour interval for each category\n",
    "def count_articles_in_interval(timestamp, timestamps, topics, category, interval=one_hour_in_seconds):\n",
    "    start_interval = timestamp - interval\n",
    "    end_interval = timestamp + interval\n",
    "    return np.sum((timestamps >= start_interval) & (timestamps <= end_interval) & (topics == category))\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(df,output_name,):\n",
    "    if output_name == \"train_df.csv\":\n",
    "        df = df.dropna(subset=['topics'])\n",
    "    ##DROP CATEGORY if exists\n",
    "    if 'category' in df.columns:\n",
    "        df = df.drop(columns=['category'])\n",
    "    \n",
    "    # Check if CUDA is available and set the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Print GPU and CUDA details if available\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EMBEDDIA/sloberta\")\n",
    "    model = AutoModel.from_pretrained(\"EMBEDDIA/sloberta\").to(device)\n",
    "    #print model size\n",
    "    def combine_columns(row):\n",
    "        title = row['title']\n",
    "        lead = row['lead']\n",
    "        paragraphs = \" \".join(row['paragraphs']) if isinstance(row['paragraphs'], list) else row['paragraphs']\n",
    "        keywords = \", \".join(row['keywords']) if isinstance(row['keywords'], list) else row['keywords']\n",
    "        gpt_keywords = \", \".join(row['gpt_keywords']) if isinstance(row['gpt_keywords'], list) else row['gpt_keywords']\n",
    "        \n",
    "        combined_text = f\"Title: {title}\\n\\nLead: {lead}\\n\\nContent: {paragraphs}\\n\\nKeywords: {keywords}\\n\\nGPT Keywords: {gpt_keywords}\"\n",
    "        return combined_text\n",
    "    \n",
    "    def category_calculation(df):\n",
    "    # Load the DataFrame\n",
    "    \n",
    "        # Function to extract categories from the URL\n",
    "        def extract_category(url):\n",
    "            # Split the URL on \"/\"\n",
    "            parts = url.split('/')\n",
    "            # Find the index for \".si\"\n",
    "            index = parts.index('www.rtvslo.si') + 1\n",
    "            # Return the next two parts joined by \"/\"\n",
    "            return '/'.join(parts[index:index+2])\n",
    "\n",
    "        # Apply the function to the 'url' column\n",
    "        df['category_new'] = df['url'].apply(extract_category)\n",
    "\n",
    "        # Count the unique values in the 'category' column\n",
    "        category_counts = df['category_new'].value_counts()\n",
    "\n",
    "        # Calculate the 0.5% threshold of the total dataset size\n",
    "        threshold = len(df) * 0.005\n",
    "\n",
    "        # Filter categories that appear more than the threshold\n",
    "        significant_categories = category_counts[category_counts > threshold].index.tolist()\n",
    "\n",
    "        # Define a function to adjust category based on significance\n",
    "        def adjust_category(category):\n",
    "            # Check if the category is significant\n",
    "            if category not in significant_categories:\n",
    "                # If not, return the higher-level hierarchy\n",
    "                return category.split('/')[0]\n",
    "            else:\n",
    "                # If it is, return the category as is\n",
    "                return category\n",
    "\n",
    "        # Apply the function to adjust categories\n",
    "        df['category_new'] = df['category_new'].apply(adjust_category)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def combine_columns_lead_title(row):\n",
    "        title = row['title']\n",
    "        lead = row['lead']  \n",
    "        keywords = \", \".join(row['keywords']) if isinstance(row['keywords'], list) else row['keywords']\n",
    "        gpt_keywords = \", \".join(row['gpt_keywords']) if isinstance(row['gpt_keywords'], list) else row['gpt_keywords']\n",
    "\n",
    "        combined_text = f\"{title} {lead} {keywords} {gpt_keywords}\"\n",
    "        return combined_text\n",
    "    \n",
    "    \n",
    "    # Function to split text into chunks\n",
    "    def split_into_chunks(text, chunk_size=512, overlap=50):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), chunk_size - overlap):\n",
    "            chunk = tokens[i:i + chunk_size]\n",
    "            chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "        return chunks\n",
    "\n",
    "    # Function to compute embeddings\n",
    "    def compute_embedding(text):\n",
    "        chunks = split_into_chunks(text)\n",
    "        chunk_embeddings = []\n",
    "        for chunk in chunks:\n",
    "            inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            chunk_embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy())\n",
    "        print(\"-\")\n",
    "        return np.mean(chunk_embeddings, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df['combined_text'] = df.apply(combine_columns, axis=1)\n",
    "    df['combined_text_lead_title'] = df.apply(combine_columns_lead_title, axis=1)\n",
    "\n",
    "    time_start = time.time()\n",
    "    # Compute embeddings for the combined text\n",
    "    df['embedding'] = df['combined_text'].apply(compute_embedding)\n",
    "    time_end = time.time()\n",
    "    print(\"Time taken to compute embeddings:\", time_end - time_start, \"seconds\")\n",
    "    \n",
    "    ##TIME TO SIN COS\n",
    "        # Convert to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Convert to Unix timestamp\n",
    "    df['unix_timestamp'] = df['date'].astype(int) / 10**9\n",
    "\n",
    "    # Normalize by subtracting the minimum value\n",
    "    min_timestamp = df['unix_timestamp'].min()\n",
    "    df['unix_timestamp'] = df['unix_timestamp'] - min_timestamp\n",
    "\n",
    "    # Define the periods (in seconds) for different cycles\n",
    "    seconds_in_day = 24 * 60 * 60\n",
    "    seconds_in_week = 7 * seconds_in_day\n",
    "    seconds_in_month = 30 * seconds_in_day  # Approximation\n",
    "    seconds_in_quarter = 3 * seconds_in_month\n",
    "    seconds_in_half_year = 6 * seconds_in_month\n",
    "    seconds_in_year = 12 * seconds_in_month\n",
    "\n",
    "    # Apply sine and cosine transformations for different cycles\n",
    "    # Daily cycle\n",
    "    df['daily_sin'] = np.sin(2 * np.pi * df['unix_timestamp'] / seconds_in_day)\n",
    "    df['daily_cos'] = np.cos(2 * np.pi * df['unix_timestamp'] / seconds_in_day)\n",
    "\n",
    "    # Three-day cycle\n",
    "    df['three_day_sin'] = np.sin(2 * np.pi * df['unix_timestamp'] / (3 * seconds_in_day))\n",
    "    df['three_day_cos'] = np.cos(2 * np.pi * df['unix_timestamp'] / (3 * seconds_in_day))\n",
    "\n",
    "    # Weekly cycle\n",
    "    df['weekly_sin'] = np.sin(2 * np.pi * df['unix_timestamp'] / seconds_in_week)\n",
    "    df['weekly_cos'] = np.cos(2 * np.pi * df['unix_timestamp'] / seconds_in_week)\n",
    "\n",
    "    # Monthly cycle\n",
    "    df['monthly_sin'] = np.sin(2 * np.pi * df['unix_timestamp'] / seconds_in_month)\n",
    "    df['monthly_cos'] = np.cos(2 * np.pi * df['unix_timestamp'] / seconds_in_month)\n",
    "\n",
    "    # Quarterly cycle\n",
    "    df['quarterly_sin'] = np.sin(2 * np.pi * df['unix_timestamp'] / seconds_in_quarter)\n",
    "    df['quarterly_cos'] = np.cos(2 * np.pi * df['unix_timestamp'] / seconds_in_quarter)\n",
    "\n",
    "    # Half-year cycle\n",
    "    df['half_yearly_sin'] = np.sin(2 * np.pi * df['unix_timestamp'] / seconds_in_half_year)\n",
    "    df['half_yearly_cos'] = np.cos(2 * np.pi * df['unix_timestamp'] / seconds_in_half_year)\n",
    "\n",
    "    ##NUMBER OF FIGURES\n",
    "    df['figure_count'] = df['figures'].apply(len)\n",
    "    \n",
    "    ##NUMBER OF PARAGRAPHS\n",
    "    df['paragraph_count'] = df['paragraphs'].apply(len)\n",
    "\n",
    "    \n",
    "    ##ARTICLE LENGTH\n",
    "    df['article_word_count'] = df['combined_text'].apply(lambda text: len(text.split()))\n",
    "    \n",
    "    ##TITLE LENGTH\n",
    "    df['title_word_count'] = df['title'].apply(lambda text: len(text.split()))\n",
    "    #DOES TITLE CONTAIN ! OR ?  Create features indicating the presence of '?' and '!' in the title\n",
    "    df['artquestion'] = df['title'].apply(lambda x: '?' in x)\n",
    "    df['artexclaim'] = df['title'].apply(lambda x: '!' in x)\n",
    "    \n",
    "    #ARTICLES IN ONE HOUR    \n",
    "\n",
    "    try:\n",
    "        # Try to open the file and load the list\n",
    "        with open(\"unique_col.json\", 'r') as file:\n",
    "            unique_topics = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, create the list and save it\n",
    "        unique_topics = list(df['topics'].unique())\n",
    "        with open(\"unique_col.json\", 'w') as file:\n",
    "            json.dump(unique_topics, file)\n",
    "        \n",
    "    \n",
    "    # Iterate over each unique topic and calculate the count in the one-hour interval\n",
    "    for topic in unique_topics:\n",
    "        df[f'{topic}_count_in_one_hour'] = df.apply(\n",
    "            lambda row: count_articles_in_interval(row['unix_timestamp'], df['unix_timestamp'], df['topics'], topic),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    timestart = time.time()\n",
    "    # Apply functions to count different types of entities\n",
    "    # Process the text and save entities\n",
    "    # Apply the NLP pipeline and process the text\n",
    "    df['doc'] = df['combined_text_lead_title'].apply(nlp)\n",
    "    df[['entities', 'lemmatized_lead_title', 'ne_loc_cnt', 'ne_per_cnt', 'ne_org_cnt', 'ne_misc_cnt']] = df['doc'].apply(lambda doc: pd.Series(process_text(doc)))\n",
    "\n",
    "\n",
    "    timeend = time.time()\n",
    "    print(\"Time taken to count entities-Spacy bert task:\", timeend - timestart, \"seconds\")\n",
    "\n",
    "    #CATEGORY NEW CALCULATION\n",
    "    category_calculation(df)\n",
    "\n",
    "\n",
    "    #THE SAME df BUT SORT COLUMNS ACCORDING TO THE ALPHABET\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    df.to_csv(output_name, index=False) \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train_df=pd.read_json(\"rtvslo_train.json\")\n",
    "test_df=pd.read_json(\"rtvslo_test.json\")\n",
    "#preprocess(train_df,\"train_df.csv\")\n",
    "preprocess(test_df,\"test_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START OF TRAINING PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_embeddings(df, drop_original=True):\n",
    "    # Assuming each embedding has the same dimension\n",
    "    embedding_dim = len(df.iloc[0]['embedding'])\n",
    "    \n",
    "    # Create a new column for each dimension of the embedding\n",
    "    for i in range(embedding_dim):\n",
    "        df[f'emb_{i}'] = df['embedding'].apply(lambda x: x[i])\n",
    "\n",
    "    # Optionally drop the 'embedding' column\n",
    "    if drop_original:\n",
    "        df.drop('embedding', axis=1, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import TransformerMixin\n",
    "#implement mopdelimport pandas as pd\n",
    "class ColumnDropper(TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.columns, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def divide_and_save_by_topic(df, topic_column=\"topics\",name=\"X_train\"):\n",
    "    \"\"\"\n",
    "    Divides a DataFrame into multiple DataFrames based on a specified topic column and saves each one to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame to be divided.\n",
    "    topic_column (str): The column name in df which contains the topic names.\n",
    "    \"\"\"\n",
    "    # Get unique topics from the topic column\n",
    "    unique_topics = df[topic_column].unique()\n",
    "    \n",
    "    # Loop through each unique topic, create a DataFrame, and save it to a CSV file\n",
    "    for topic in unique_topics:\n",
    "        # Filter DataFrame by topic\n",
    "        topic_df = df[df[topic_column] == topic]\n",
    "        \n",
    "        # Create a file name using the topic name\n",
    "        file_name = f\"{name}_{topic}.csv\"\n",
    "        \n",
    "        # Save DataFrame to CSV\n",
    "        topic_df.to_csv(file_name, index=False)\n",
    "        print(f\"Saved: {file_name}\")\n",
    "\n",
    "# Example usage:\n",
    "X_train = pd.read_csv(\"train_df_final.csv\")\n",
    "divide_and_save_by_topic(X_train,name=\"X_train\")\n",
    "X_test=pd.read_csv(\"test_df_final.csv\")\n",
    "divide_and_save_by_topic(X_test,name=\"X_test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOT FEATURE DISTRIBUITONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the DataFrame\n",
    "\n",
    "train_df = read_csv_with_embeddings(\"train_df.csv\")\n",
    "\n",
    "def plot_feature_distributions(dataframe):\n",
    "    # Select numeric columns only and exclude columns starting with 'emb_'\n",
    "    numeric_columns = [col for col in dataframe.select_dtypes(include=['number']).columns if not col.startswith('emb_')]\n",
    "    n_rows = len(numeric_columns)\n",
    "    \n",
    "    # Adjust figure size based on the number of rows\n",
    "    fig, axes = plt.subplots(n_rows, 1, figsize=(10, 5 * n_rows), squeeze=False)  # squeeze=False makes sure axes is always a 2D array\n",
    "    fig.tight_layout(pad=5.0)\n",
    "\n",
    "    for i, col in enumerate(numeric_columns):\n",
    "        ax = axes[i, 0]  # \n",
    "        sns.histplot(dataframe[col], kde=True, ax=ax)  # kde=True adds a kernel density estimate to smooth the histogram\n",
    "        ax.set_title(f'Distribution of {col}', fontsize=16)\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Frequency')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plotting the distributions for non-'emb_' prefixed columns\n",
    "plot_feature_distributions(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALGORITHEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE GRID SEARCH\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from itertools import product\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Function to search over the grid\n",
    "def custom_grid_search(pipeline, param_grid, X_train, y_train, X_test, y_test):\n",
    "    best_mae = np.inf\n",
    "    best_params = {}\n",
    "    \n",
    "    # Create a Cartesian product of the parameter grid\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    for v in product(*values):\n",
    "        time_start = time.time()\n",
    "        parameters = dict(zip(keys, v))\n",
    "        pipeline.set_params(**parameters)\n",
    "        \n",
    "        # Fit the model on the entire training set\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the test set\n",
    "        predictions = pipeline.predict(X_test)\n",
    "        \n",
    "        # Calculate the MAE\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        \n",
    "        # Update the best MAE and best parameters if current MAE is lower\n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            best_params = parameters\n",
    "        time_end = time.time()\n",
    "        print(\"Time taken for one iteration:\", time_end - time_start,\"params:\",parameters, \"seconds\",\"MAE:\",mae)\n",
    "    \n",
    "    return best_mae, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "class MultiLabelBinarizerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.mlb.fit(X[self.column])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        data_encoded = self.mlb.transform(X[self.column])\n",
    "        df_encoded = pd.DataFrame(data_encoded, index=X.index, columns=[f\"{self.column}_{cls}\" for cls in self.mlb.classes_])\n",
    "        X = X.drop(columns=self.column)\n",
    "        print(\"num of features\",len(df_encoded.columns)+len(X.columns))\n",
    "        return pd.concat([X, df_encoded], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPORTS XGBOOST\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from lemmagen3 import Lemmatizer\n",
    "import difflib\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# Load the DataFrame\n",
    "\n",
    "train_df=...\n",
    "test_df=...\n",
    "print(list(train_df.columns))\n",
    "\n",
    " \n",
    "print(\"TRAIN NUM FEATURES\",len(train_df.columns))\n",
    "\n",
    "\n",
    "\n",
    "y_train = train_df['n_comments']\n",
    "X_train = train_df.drop('n_comments', axis=1)\n",
    "y_test = test_df['n_comments']\n",
    "X_test = test_df.drop('n_comments', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "columns_to_drop = ['url', 'authors', 'date', 'title', 'paragraphs', 'figures', 'lead',\n",
    "                   'topics', 'keywords', 'gpt_keywords', 'id', 'combined_text',\n",
    "                   'combined_text_lead_title', 'unix_timestamp',\n",
    "                   'doc', 'lemmatized_lead_title', 'ne_misc_cnt','category_new']\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    # Assuming 'ColumnDropper' is defined to drop columns specified in 'columns_to_drop'\n",
    "    ('drop_columns', ColumnDropper(columns_to_drop)),\n",
    "    # Add MultiLabelBinarizer for the \"entities\" column\n",
    "    ('multi_label_encoder', ColumnTransformer(\n",
    "        [('multi_label_binarizer', MultiLabelBinarizerTransformer('entities'), ['entities'])],\n",
    "        remainder='passthrough')),\n",
    "\n",
    "    ('xgb_regressor', xgb.XGBRegressor(objective='reg:squarederror'))  # Using squared error as the loss for regression\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'xgb_regressor__n_estimators': [150, 300],\n",
    "    'xgb_regressor__max_depth': [ 30],#5\n",
    "    'xgb_regressor__learning_rate': [ 0.05],#0.1, \n",
    "    'xgb_regressor__subsample': [0.7],#0.9\n",
    "    'xgb_regressor__objective': ['reg:squarederror']\n",
    "}\n",
    "\n",
    "# Run the custom grid search\n",
    "best_mae, best_params = custom_grid_search(pipeline, param_grid, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(f\"Best MAE: {best_mae}\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINEAR REGRESSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from lemmagen3 import Lemmatizer\n",
    "import difflib\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_df =...\n",
    "test_df = ...\n",
    "\n",
    "\n",
    "y_train = train_df['n_comments']\n",
    "X_train = train_df.drop('n_comments', axis=1)\n",
    "y_test = test_df['n_comments']\n",
    "X_test = test_df.drop('n_comments', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "columns_to_drop = ['url', 'authors', 'date', 'title', 'paragraphs', 'figures', 'lead',\n",
    "                   'topics', 'keywords', 'gpt_keywords', 'id', 'combined_text',\n",
    "                   'combined_text_lead_title', 'unix_timestamp',\n",
    "                   'doc', 'entities', 'lemmatized_lead_title', 'ne_misc_cnt','category_new']\n",
    "\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.1,0.5,0.7,0.3, 1.0, 10.0, 100.0,0.0000001,500,1000,5000,10000,]  # Regularization strengths\n",
    "    \n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('drop_columns', ColumnDropper(columns_to_drop)),\n",
    "    ('scaler', StandardScaler()),  # This is especially useful for regularization\n",
    "    ('regressor', Lasso())  # Can be switched to LinearRegression() if no regularization is needed\n",
    "])\n",
    "# Running the custom grid search\n",
    "best_mae, best_params = custom_grid_search(pipeline, param_grid, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(f\"Best MAE: {best_mae}\")\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTUNA AND NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTUNA FOR SEARCHING BEST PARAMETERS\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from lemmagen3 import Lemmatizer\n",
    "import difflib\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "columns_to_drop = ['url', 'authors', 'date', 'title', 'paragraphs', 'figures', 'lead',\n",
    "                   'topics', 'keywords', 'gpt_keywords', 'id', 'combined_text',\n",
    "                   'combined_text_lead_title', 'unix_timestamp',\n",
    "                   'doc', 'entities', 'lemmatized_lead_title', 'ne_misc_cnt','category_new']#,'sentiment_negative', 'sentiment_neutral', 'sentiment_positive'\n",
    "\n",
    "\n",
    "# Load the DataFrame\n",
    "df = read_csv_with_embeddings(\"train_df.csv\")\n",
    "df=unpack_embeddings(df)\n",
    "\n",
    "df_test=...\n",
    "df_test=...\n",
    "\n",
    "\n",
    "\n",
    "df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "X_train = df.drop('n_comments', axis=1).values\n",
    "y_train = df['n_comments'].values\n",
    "#take sqrt of y_train\n",
    "y_train=np.sqrt(y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_test.drop(columns_to_drop, axis=1, inplace=True)\n",
    "X_test = df_test.drop('n_comments', axis=1).values\n",
    "y_test = df_test['n_comments'].values\n",
    "\n",
    "\n",
    "# Standard scaling of features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_val=X_test\n",
    "y_val=y_test\n",
    "\n",
    "\n",
    "# Converting data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "globalBestMae=100000.0\n",
    "\n",
    "\n",
    "def initialize_weights_xavier(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    INITIAL_SEED = 1\n",
    "    random.seed(INITIAL_SEED)\n",
    "    np.random.seed(INITIAL_SEED)\n",
    "    torch.manual_seed(INITIAL_SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(INITIAL_SEED)\n",
    "        torch.cuda.manual_seed_all(INITIAL_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed seed value for initialization\n",
    "\n",
    "    # Suggest the number of layers and their sizes\n",
    "    n_layers = trial.suggest_int('n_layers', 2, 4)\n",
    "    layers = [trial.suggest_int('n_units_l{}'.format(i), 20, 140) for i in range(n_layers)]\n",
    "    dropout_rates = [trial.suggest_uniform('dropout_l{}'.format(i), 0, 0.5) for i in range(n_layers)]  # Dropout rate for each layer\n",
    "\n",
    "    # Calculate the minimum size among training and validation datasets\n",
    "    min_dataset_size = min(len(X_train), len(X_val))\n",
    "\n",
    "    # Create a list of possible batch sizes smaller than the minimum dataset size\n",
    "    possible_batch_sizes = [size for size in range(1, 1025) if size < min_dataset_size]    \n",
    "    # Ensure there's always at least one valid batch size option\n",
    "    if not possible_batch_sizes:\n",
    "        possible_batch_sizes = [min_dataset_size]  # Fallback to the smallest dataset size if all standard options are too large\n",
    "\n",
    "    # Suggest batch size from the filtered list\n",
    "    batch_size = trial.suggest_categorical('batch_size', possible_batch_sizes)\n",
    "\n",
    "    # Building the neural network model\n",
    "    model = nn.Sequential()\n",
    "    input_size = X_train.shape[1]\n",
    "    for i in range(n_layers):\n",
    "        output_size = layers[i]\n",
    "        model.add_module(f\"linear{i}\", nn.Linear(input_size, output_size))\n",
    "        model.add_module(f\"relu{i}\", nn.ReLU())\n",
    "        model.add_module(f\"dropout{i}\", nn.Dropout(dropout_rates[i]))\n",
    "        input_size = output_size  # update input size for the next layer\n",
    "    model.add_module(\"output\", nn.Linear(layers[-1], 1))\n",
    "    \n",
    "    model.apply(initialize_weights_xavier)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(INITIAL_SEED)\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    # Moving model to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    # Setup optimizer and loss function\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-1)  # Weight decay for regularization\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=weight_decay)\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    # Early stopping parameters\n",
    "    best_val_mae = float('inf')\n",
    "    the_best_val_mae=float('inf')\n",
    "    best_epoch = 0\n",
    "    patience = 33  # Number of epochs to tolerate no improvement\n",
    "    min_delta = 0.001  # Minimum change in the monitored quantity to qualify as an improvement\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Training loop\n",
    "    time_start = time.time()\n",
    "    for epoch in range(600):  # Maximum number of epochs\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_mae = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                if torch.cuda.is_available():\n",
    "                    batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
    "                output = model(batch_x)\n",
    "                output=output**2\n",
    "                val_mae += criterion(output, batch_y).item()\n",
    "        val_mae /= len(val_loader)\n",
    "\n",
    "\n",
    "        if the_best_val_mae>val_mae:\n",
    "            the_best_val_mae=val_mae\n",
    "            best_epoch=epoch\n",
    "\n",
    "        global globalBestMae\n",
    "        if val_mae<globalBestMae:\n",
    "            globalBestMae=val_mae\n",
    "            with open(\"optuna_optimization_best_mae.txt\", \"w\") as file:\n",
    "                file.write(f\"Best Validation MAE: {the_best_val_mae}\\n\")\n",
    "                file.write(f\"Best Epoch: {best_epoch}\\n\")\n",
    "                file.write(f\"Parameters: {trial.params}\\n\")\n",
    "                file.write(f\"Epoch: {epoch}  potem ne pozabi dodat 1\\n\")\n",
    "            torch.save(model.state_dict(), 'best_model_optuna.pth')\n",
    "            \n",
    "        \n",
    "        if best_val_mae - val_mae > min_delta:\n",
    "            best_val_mae=val_mae\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}. Best epoch: {best_epoch} with MAE: {best_val_mae}\")\n",
    "            break\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    trial.set_user_attr('best_val_mae', the_best_val_mae)\n",
    "    trial.set_user_attr('best_epoch', best_epoch)\n",
    "    trial.set_user_attr('seed', INITIAL_SEED)\n",
    "\n",
    "    time_end = time.time()\n",
    "    print(f\"Trial took {time_end - time_start} seconds|\")\n",
    "    return the_best_val_mae\n",
    "\n",
    "\n",
    "# Set the seed for Optuna\n",
    "sampler = optuna.samplers.TPESampler(seed=1)\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "study.optimize(objective, timeout=100000)  # Adjust timeout as needed\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"Best Validation MAE: {trial.value}\")\n",
    "print(\"Best Epoch: \", trial.user_attrs['best_epoch'])\n",
    "#print best seed\n",
    "print(\"Best Seed: \", trial.user_attrs['seed'])\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTUNA FOR SUBSET OF TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTUNA FOR SEARCHING BEST PARAMETERS NN@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from lemmagen3 import Lemmatizer\n",
    "import difflib\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "columns_to_drop = ['url', 'authors', 'date', 'title', 'paragraphs', 'figures', 'lead',\n",
    "                   'topics', 'keywords', 'gpt_keywords', 'id', 'combined_text',\n",
    "                   'combined_text_lead_title', 'unix_timestamp',\n",
    "                   'doc', 'entities', 'lemmatized_lead_title', 'ne_misc_cnt']#,,'category_new','sentiment_negative', 'sentiment_neutral', 'sentiment_positive'\n",
    "\n",
    "globalBestMae=100000.0\n",
    "def searchTopic(topic,df,secondsMax):\n",
    "\n",
    "\n",
    "   #only select rows that have topics equal to topic\n",
    "    df=df[df['topics']==topic]\n",
    "    \n",
    "    test_size = 0.05\n",
    "    split_index = int(df.shape[0] * (1 - test_size))\n",
    "\n",
    "    df = df.iloc[:split_index]\n",
    "    df_test = df.iloc[split_index:]\n",
    "\n",
    "    \n",
    "\n",
    "    ##################################################\n",
    "    ##################################################  \n",
    "         # Initialize the OneHotEncoder\n",
    "    # Concatenate 'category_new' columns from both DataFrames for fitting the encoder\n",
    "    combined_categories = pd.concat([df[['category_new']], df_test[['category_new']]])\n",
    "\n",
    "    # Initialize the OneHotEncoder\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "    # Fit the encoder on the combined data\n",
    "    encoder.fit(combined_categories)\n",
    "\n",
    "    # Transform the 'category_new' column for both DataFrames\n",
    "    encoded_df = pd.DataFrame(encoder.transform(df[['category_new']]), columns=encoder.get_feature_names_out(['category_new']))\n",
    "    encoded_df_test = pd.DataFrame(encoder.transform(df_test[['category_new']]), columns=encoder.get_feature_names_out(['category_new']))\n",
    "\n",
    "    # Drop the original 'category_new' column and concatenate the new one-hot encoded columns\n",
    "    df = df.drop('category_new', axis=1).reset_index(drop=True)\n",
    "    df_test = df_test.drop('category_new', axis=1).reset_index(drop=True)\n",
    "\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "    df_test = pd.concat([df_test, encoded_df_test], axis=1)  \n",
    "\n",
    "#####################################\n",
    "#####################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    X_train = df.drop('n_comments', axis=1).values\n",
    "    y_train = df['n_comments'].values\n",
    "    #take sqrt of y_train\n",
    "    y_train=np.sqrt(y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df_test.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    X_test = df_test.drop('n_comments', axis=1).values\n",
    "    y_test = df_test['n_comments'].values\n",
    "\n",
    "\n",
    "\n",
    "    # Standard scaling of features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    X_val=X_test\n",
    "    y_val=y_test\n",
    "\n",
    "\n",
    "    # Converting data to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    " \n",
    "    global globalBestMae\n",
    "    globalBestMae=100000.0\n",
    "\n",
    "\n",
    "    def initialize_weights_xavier(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Define the objective function for Optuna\n",
    "    def objective(trial):\n",
    "        INITIAL_SEED = 1\n",
    "        random.seed(INITIAL_SEED)\n",
    "        np.random.seed(INITIAL_SEED)\n",
    "        torch.manual_seed(INITIAL_SEED)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(INITIAL_SEED)\n",
    "            torch.cuda.manual_seed_all(INITIAL_SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "        # Suggest the number of layers and their sizes\n",
    "        n_layers = trial.suggest_int('n_layers', 2, 4)\n",
    "        layers = [trial.suggest_int('n_units_l{}'.format(i), 20, 140) for i in range(n_layers)]\n",
    "        dropout_rates = [trial.suggest_uniform('dropout_l{}'.format(i), 0.008, 0.5) for i in range(n_layers)]  # Dropout rate for each layer\n",
    "\n",
    "        # Calculate the minimum size among training and validation datasets\n",
    "        min_dataset_size = min(len(X_train), len(X_val))\n",
    "\n",
    "        # Create a list of possible batch sizes smaller than the minimum dataset size\n",
    "        possible_batch_sizes = [size for size in range(1, 1025) if size < min_dataset_size]        \n",
    "        # Ensure there's always at least one valid batch size option\n",
    "        if not possible_batch_sizes:\n",
    "            possible_batch_sizes = [min_dataset_size]  # Fallback to the smallest dataset size if all standard options are too large\n",
    "\n",
    "        # Suggest batch size from the filtered list\n",
    "        batch_size = trial.suggest_categorical('batch_size', possible_batch_sizes)\n",
    "\n",
    "        # Building the neural network model\n",
    "        model = nn.Sequential()\n",
    "        input_size = X_train.shape[1]\n",
    "        for i in range(n_layers):\n",
    "            output_size = layers[i]\n",
    "            model.add_module(f\"linear{i}\", nn.Linear(input_size, output_size))\n",
    "            model.add_module(f\"relu{i}\", nn.ReLU())\n",
    "            model.add_module(f\"dropout{i}\", nn.Dropout(dropout_rates[i]))\n",
    "            input_size = output_size  # update input size for the next layer\n",
    "        model.add_module(\"output\", nn.Linear(layers[-1], 1))\n",
    "        \n",
    "        model.apply(initialize_weights_xavier)\n",
    "\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(INITIAL_SEED)\n",
    "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "        val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "        # Moving model to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            model.cuda()\n",
    "        else:\n",
    "            print(\"Using CPU\")\n",
    "\n",
    "        # Setup optimizer and loss function\n",
    "        weight_decay = trial.suggest_loguniform('weight_decay', 3*1e-5, 1e-1)  # Weight decay for regularization\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=weight_decay)\n",
    "        criterion = nn.L1Loss()\n",
    "\n",
    "        # Early stopping parameters\n",
    "        best_val_mae = float('inf')\n",
    "        the_best_val_mae=float('inf')\n",
    "        best_epoch = 0\n",
    "        patience = 33  # Number of epochs to tolerate no improvement\n",
    "        min_delta = 0.001  # Minimum change in the monitored quantity to qualify as an improvement\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        # Training loop\n",
    "        time_start = time.time()\n",
    "        for epoch in range(600):  # Maximum number of epochs\n",
    "            model.train()\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                if torch.cuda.is_available():\n",
    "                    batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                output = model(batch_x)\n",
    "                loss = criterion(output, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            val_mae = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    if torch.cuda.is_available():\n",
    "                        batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
    "                    output = model(batch_x)\n",
    "                    output=output**2\n",
    "                    val_mae += criterion(output, batch_y).item()\n",
    "            val_mae /= len(val_loader)\n",
    "\n",
    "\n",
    "            if the_best_val_mae>val_mae:\n",
    "                the_best_val_mae=val_mae\n",
    "                best_epoch=epoch\n",
    "            best_model_name=\"best_model_optuna_\"+topic+\".pth\"\n",
    "            file_name=\"WITH_ONEHOT_optuna_optimization_current_mae_\"+topic+\".txt\"\n",
    "            global globalBestMae\n",
    "            if val_mae<globalBestMae:\n",
    "                globalBestMae=val_mae\n",
    "                with open(file_name, \"a\") as file:\n",
    "                    file.write(f\"Best Validation MAE: {globalBestMae}\\n\")\n",
    "                    file.write(f\"Best Epoch: {best_epoch}\\n\")\n",
    "                    file.write(f\"Parameters: {trial.params}\\n\")\n",
    "                    file.write(f\"Epoch: {epoch}  potem ne pozabi dodat 1\\n\")\n",
    "                torch.save(model.state_dict(), best_model_name)\n",
    "                \n",
    "            \n",
    "            if best_val_mae - val_mae > min_delta:\n",
    "                best_val_mae=val_mae\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            # Early stopping\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best epoch: {best_epoch} with MAE: {best_val_mae}\")\n",
    "                break\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        trial.set_user_attr('best_val_mae', the_best_val_mae)\n",
    "        trial.set_user_attr('best_epoch', best_epoch)\n",
    "        trial.set_user_attr('seed', INITIAL_SEED)\n",
    "\n",
    "        time_end = time.time()\n",
    "        print(f\"Trial took {time_end - time_start} seconds|\")\n",
    "        return the_best_val_mae\n",
    "\n",
    "\n",
    "    # Set the seed for Optuna\n",
    "    sampler = optuna.samplers.TPESampler(seed=1)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "    study.optimize(objective, timeout=secondsMax)  # Adjust timeout as needed\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Best Validation MAE: {trial.value}\")\n",
    "    print(\"Best Epoch: \", trial.user_attrs['best_epoch'])\n",
    "    #print best seed\n",
    "    print(\"Best Seed: \", trial.user_attrs['seed'])\n",
    "    print(\"Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "\n",
    "#suppre fragmented df warning\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "df = read_csv_with_embeddings(\"train_df.csv\")\n",
    "df=unpack_embeddings(df)\n",
    "\n",
    "topics_list = [...]\n",
    "times=[...]\n",
    "times=[i*60 for i in times]\n",
    "print(times)\n",
    "for i in range(len(topics_list)):\n",
    "    topic=topics_list[i]\n",
    "    print(\"WORKING ON TOPIC: \",topic)\n",
    "    secondsMax=times[i]\n",
    "    searchTopic(topic,df,secondsMax)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uozp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
